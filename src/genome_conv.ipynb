{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "# import torchvision.datasets as datasets\n",
    "# import torchvision.transforms as transforms\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "sys.path.append('python_scripts')\n",
    "# local imports\n",
    "from python_scripts import file_mgmt, mod_seqs, constants, ai_funcs\n",
    "from python_scripts.networks.AlexNet import AlexNet\n",
    "from SeqsData import SeqsData\n",
    "\n",
    "# Author(s): Zachery Linscott and Dr. Manas Jyoti Das"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if GPU is available or else default to CPU usage.\n",
    "device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One hot encode the data\n",
    "\n",
    "print(\"Reading in test, train, and validation files\")\n",
    "test = file_mgmt.read_file(constants.TEST) # 20% of the original data\n",
    "train = file_mgmt.read_file(constants.TRAIN) # 70% of the original data\n",
    "validation = file_mgmt.read_file(constants.VALID) # 5% of the original data\n",
    "\n",
    "print(\"Splitting the test, train and validation data into lists\")\n",
    "test = [line.split() for line in test]\n",
    "train = [line.split() for line in train]\n",
    "validation = [line.split() for line in validation]\n",
    "print(\"\\nDone\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frac = 1\n",
    "# for testing purposes, use only small slices of datasets\n",
    "print(f\"One hot encoding {frac * 100}% of the test data\\n\")\n",
    "test_one_hot = mod_seqs.one_hot_encode(test[:int(len(test) * frac)])\n",
    "# np.save(constants.ONE_HOT_TEST, test_one_hot)\n",
    "\n",
    "print(f\"One hot encoding {frac * 100}% of the train data\\n\")\n",
    "train_one_hot = mod_seqs.one_hot_encode(train[:int(len(train) * frac)])\n",
    "# np.save(constants.ONE_HOT_TRAIN, train_one_hot)\n",
    "\n",
    "print(f\"One hot encoding {frac * 100}% of the validation data\\n\")\n",
    "valid_one_hot = mod_seqs.one_hot_encode(validation[:int(len(validation) * frac)])\n",
    "# np.save(constants.ONE_HOT_VALID, valid_one_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create test dataset to be fed into the dataloader\n",
    "np.transpose(test_one_hot)\n",
    "sd_test = SeqsData(test_one_hot)\n",
    "\n",
    "# create train dataset to be fed into dataloader\n",
    "np.transpose(train_one_hot)\n",
    "sd_train = SeqsData(train_one_hot)\n",
    "\n",
    "# create train dataset to be fed into dataloader\n",
    "np.transpose(valid_one_hot)\n",
    "sd_valid = SeqsData(valid_one_hot)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataLoader Objects are what feed the network the data by means of tensor stacks defined in the SeqsData class.\n",
    "test_dataloader = DataLoader(sd_test, batch_size=512, shuffle=True)\n",
    "train_dataloader = DataLoader(sd_train, batch_size=512, shuffle=True)\n",
    "valid_dataloader = DataLoader(sd_valid, batch_size=512, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform_conf = transforms.Compose([\n",
    "#     # transforms.Resize((224,224)),\n",
    "#     transforms.ToTensor(),\n",
    "#     transforms.Normalize((0.1307,),(0.3081,))\n",
    "# ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiating the model and assigning an optimizer to the model and creating a loss function\n",
    "model = AlexNet()\n",
    "model.load_state_dict(torch.load(\"saved_models/calexnet8.pth\"))\n",
    "model = model.to(device)\n",
    "optimizer = optim.Adam(params=model.parameters(),lr=0.0001)\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__=='__main__':\n",
    "    seed=42\n",
    "    EPOCHS = 10 # run for 10 epochs at least\n",
    "    \n",
    "    for epoch in range(1, EPOCHS + 1):\n",
    "        ai_funcs.train(model, device, train_dataloader, optimizer,epoch, loss_fn)\n",
    "        ai_funcs.test(model,device,test_dataloader)\n",
    "        ai_funcs.validation(model, device, valid_dataloader)\n",
    "        torch.save(model.state_dict(), constants.ALEXNET + f\"{epoch}\" + \".pth\") # save model for each epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = torch.load(\"zachnet.pth\")\n",
    "# print(model)\n",
    "# model.eval()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
